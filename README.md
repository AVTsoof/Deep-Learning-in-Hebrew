# Deep-Learning-in-Hebrew
למידת מכונה ולמידה עמוקה בעברית

Add a star if the repository helped you :)

![MDLH](https://user-images.githubusercontent.com/56107590/113211481-2d641380-927e-11eb-9e85-8faef47de082.png)

For any issue please contact us at Avrahamsapir1@gmail.com.

## People

[Avraham Raviv](https://www.linkedin.com/in/avraham-raviv-47b3b5158/)

[Mike Erlihson](https://www.linkedin.com/in/michael-mike-erlihson-8208616/)

[Jeremy Rutman](https://he.rutmanip.com/)

[Nava (Reinitz) Leibovich](https://www.linkedin.com/in/nava-leibovich/?originalSubdomain=il)

[Or Shemesh](https://www.linkedin.com/in/or-shemesh-a3b0b117b/)

[Ron Levy](https://www.linkedin.com/in/ronlevy120/)

## Table of contents

Part I:

* [1. Introducion to Machine Learning](#1-Introducion-to-Machine-Learning)
* [2. Machine Learning Algorithms](#2-Machine-Learning-Algorithms)

Part II:
* [3. Linear Neural Networks (Regression problems)](#3-Linear-Neural-Networks-(Regression-problems))
* [4. Deep Neural Networks](#4-Deep-Neural-Networks)
* [5. Convolutional Neural Networks](#5-Convolutional-Neural-Networks)
* [6. Recurrent Neural Networks](#6-Recurrent-Neural-Networks)
* [7. Deep Generative Models](#7-Deep-Generative-Models)
* [8. Attention Mechanism](#8-Attention-Mechanism)

Part III:
* [9. Computer Vision](#9-Computer-Vision)
* [10. Natural Language Process](#10-Natural-Language-Process)
* [11. Reinforcement Learning](#11-Reinforcement-Learning)


* [References](#References)

   
   
--------------------------------------
--------------------------------------

## 1. Introducion to Machine Learning
#### 1.1 What is Machine Learning?

- [x] 1.1.1 The Basic Concept

- [x] 1.1.2 Data, Tasks and Learning

#### 1.2 Applied Math

- [x] 1.2.1 Linear Algebra

- [x] 1.2.2 Calculus

- [x] 1.2.3 Probability

## 2. Machine Learning Algorithms
#### 2.1 Supervised Learning Algorithms
- [x] 2.1.1 Support Vector Machines (SVM)

- [x] 2.1.2 Naïve Bayes

- [x] 2.1.3 K-nearest neighbors (K-NN)

- [x] 2.1.4 Qadratic\Linear Discriminant Analysis (QDA\LDA)

- [ ] 2.1.5 Decision Trees

#### 2.2 Unsupervised Learning Algorithms
- [x] 2.2.1 K-means

- [x] 2.2.2 Mixture Models

- [x] 2.2.3 Expectation–maximization (EM)

- [x] 2.2.4 Hierarchical Clustering

- [x] 2.2.5 Local Outlier Factor



#### 2.3 Dimensionally Reduction
- [x] 2.3.1 Principal Components Analysis (PCA)

- [x] 2.3.2 t-distributed Stochastic Neighbor Embedding (t-SNE)

- [ ] 2.3.3 Uniform Manifold Approximation and Projection (UMAP)


#### 2.4	Ensemble Learning

- [x] 2.4.1 Introduction to Ensemble Learning
 
- [x] 2.4.2 Bagging
 
- [x] 2.4.3 Boosting


## 3. Linear Neural Networks (Regression problems)

#### 3.1	Linear Regression
- [x] 3.1.1 The Basic Concept

- [x] 3.1.2 Gradient Descent	

- [x] 3.1.3 Regularization and Cross Validation	

- [x] 3.1.4 Linear Regression as Classifier	

#### 3.2	Softmax Regression	
- [x] 3.2.1 Logistic Regression	

- [x] 3.2.2 Cross Entropy and Gradient descent	

- [x] 3.2.3 Optimization	

- [x] 3.2.4 SoftMax Regression – Multi Class Logistic Regression	

- [x] 3.2.5 SoftMax Regression as Neural Network	



## 4. Deep Neural Networks	
#### 4.1	MLP – Multilayer Perceptrons	

- [x] 4.1.1 From a Single Neuron to Deep Neural Network	

- [x] 4.1.2 Activation Function	

- [x] 4.1.3 Xor	

#### 4.2	Computational Graphs and propagation	
- [x] 4.2.1 Computational Graphs	

- [x] 4.2.2 Forward and Backward propagation	
#### 4.3	Optimization	
- [x] 4.3.1 Data Normalization	

- [x] 4.3.2 Weight Initialization	

- [x] 4.3.3 Batch Normalization	

- [x] 4.3.4 Mini Batch	

- [x] 4.3.5 Gradient Descent Optimization Algorithms	
#### 4.4	Generalization	
- [x] 4.4.1 Regularization	

- [x] 4.4.2 Weight Decay	

- [x] 4.4.3 Model Ensembles and Drop Out	

- [x] 4.4.4 Data Augmentation	



## 5. Convolutional Neural Networks	

#### 5.1	Convolutional Layers	
- [x] 5.1.1 From Fully-Connected Layers to Convolutions	

- [x] 5.1.2 Padding, Stride and Dilation	

- [x] 5.1.3 Pooling	

- [x] 5.1.4 Training	

- [x] 5.1.5 Convolutional Neural Networks (LeNet)	
#### 5.2	CNN Architectures	
- [x] 5.2.1 AlexNet	

- [x] 5.2.2 VGG	

- [x] 5.2.3 GoogleNet	

- [x] 5.2.4 Residual Networks (ResNet)	

- [x] 5.2.5 Densely Connected Networks (DenseNet)	

- [x] 5.2.6 U-Net	

- [x] 5.2.7 Transfer Learning	



## 6. Recurrent Neural Networks	

 #### 6.1	Sequence Models	
- [x] 6.1.1 Recurrent Neural Networks	

- [x] 6.1.2 Learning Parameters	

#### 6.2 RNN Architectures	
- [x] 6.2.1 Long Short-Term Memory (LSTM)	

- [x] 6.2.2 Gated Recurrent Units (GRU)	

- [x] 6.2.3 Deep RNN	

- [x] 6.2.4 Bidirectional RNN

- [ ] 6.2.5 Sequence to Sequence Learning


## 7. Deep Generative Models	
#### 7.1 Variational AutoEncoder (VAE)	
- [x] 7.1.1 Dimensionality Reduction	

- [x] 7.1.2 Autoencoders (AE)	

- [x] 7.1.3 Variational AutoEncoders (VAE)	

#### 7.2 Generative Adversarial Networks (GANs)	
- [X] 7.2.1 Generator and Discriminator

- [X] 7.2.2 DCGAN

- [x] 7.2.3 Conditional GAN (cGAN)

- [X] 7.2.4 Pix2Pix

- [X] 7.2.5 CycleGAN

- [x] 7.2.6 Progressively Growing (ProGAN)

- [x] 7.2.7 StyleGAN

- [x] 7.2.8 Wasserstein GAN



#### 7.3 Auto-Regressive Generative Models
- [x] 7.3.1 PixelRNN

- [x] 7.3.2 PixelCNN

- [x] 7.3.3 Gated PixelCNN

- [x] 7.3.4 PixelCNN++


## 8. Attention Mechanism	
#### 8.1 Sequence to Sequence Learning and Attention

- [x] 8.1.1 Attention in Seq2Seq Models

- [x] 8.1.2 Bahdanau Attention and Luong Attention

#### 8.2 Transformer

- [x] 8.2.1 Positional Encoding

- [x] 8.2.2 Self-Attention Layer

- [x] 8.2.3 Multi Head Attention 

- [x] 8.2.4 Transformer End to End 

- [x] 8.2.5 Transformer Applications


## 9. Computer Vision	
#### 9.1 Object Detection

- [ ]	9.1.1 R-CNN

- [ ]	9.1.2 You Only Look Once (YOLO)

- [ ]	9.1.3 Single Shot Detector (SSD)

- [ ]	9.1.4 Spatial Pyramid Pooling (SPP-net)

- [ ]	9.1.5 Feature Pyramid Networks

- [ ]	9.1.6 Deformable Convolutional Networks

- [ ]	9.1.7 DE⫶TR: Object Detection with Transformers

#### 9.2	Segmentation

- [ ]	9.2.1 Semantic Segmentation vs. Instance Segmentation

- [ ]	9.2.2 SegNet neural network

- [ ]	9.2.3 Atrous convolutions

- [ ]	9.2.4 Atrous Spatial Pyramidal Pooling

- [ ]	9.2.5 Conditional Random Fields usage for improving final output 

- [ ]	9.2.6 See More Than Once -- Kernel-Sharing Atrous Convolution

#### 9.3 Face Recognition and Pose Estimation

- [x]	9.3.1 Face Recognition 

- [ ]	9.3.2 Pose Estimation

<!--
#### 9.4	Image Captioning	
-->
<!--
#### 9.5	Pose Estimation and Face Recognition
-->


## 10. Natural Language Process	
#### 10.1 Language Model
- [ ]	10.1.1 N-gram

- [ ]	10.1.2 Word Representation (Vectors)

- [ ]	10.1.3 Word2Vec/GloVe

- [ ]	10.1.4 ELMo - Embeddings from Language Model

- [ ]	10.1.5 Attention/Transformer (GPT)

#### 10.2 Neural Machine Translation
- [ ] 10.2.1 Neural Machine Translation by Jointly Learning to Align and Translate
 
- [ ] 10.2.2 Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
 
- [ ] 10.2.3 ConvS2S

- [ ] 10.2.4 RNMT+
 
- [ ] 10.2.5 Transformer and Transformer based models

- [ ] 10.2.6 Named Entity Recognition (NER)

- [ ] 10.2.7 Bilingual Evaluation Understudy (BLEU score)

- [ ] 10.2.8 Unsupervised Machine Translation

#### 10.3 Speech Recognition

- [ ]	10.3.1 Connectionist Temporal Classification

- [ ]	10.3.2 Listen, Attend, and Spell

- [ ]	10.3.3 Very Deep Convolutional Networks for End-to-End Speech Recognition

#### 10.4 Document Summarization

Extractive Text Summarization:
- [ ]	10.4.1 TextRank

- [ ]	10.4.2 LexRank

- [ ]	10.4.3 Luhn

- [ ]	10.4.4 Latent Semantic Analysis, LSA

- [ ]	10.4.5 KL-Sum

Abstractive Text Summarization:

- [ ]	10.4.6 T5 Transformers

- [ ]	10.4.7 BART Transformers

- [ ]	10.4.8 GPT-2 Transformers

- [ ]	10.4.9 XLM Transformers




## 11. Reinforcement Learning
#### 11.1 Introduction to RL
- [x]	11.1.1 Markov Decision Process (MDP) and RL

- [x]	11.1.2 Planning

- [x]	11.1.3 Learning Algorithms

#### 11.2 Exploration and Exploitation

#### 11.3 Planning by Dynamic Programming

#### 11.4 Policy Gradient Methods

#### 11.5 Monte-Carlo

#### 11.6 Temporal-Difference Learning

#### 11.7 Model-based algorithms

--------------------------------------
--------------------------------------

## References
[Stanford cs231](https://github.com/mbadry1/CS231n-2017-Summary)

[Machine Learning - Andrew Ng](https://www.holehouse.org/mlclass/)

[Dive into Deep Learning](http://d2l.ai/)

[Deep Learning Book](https://www.deeplearningbook.org/)










כל הזכויות שמורות @
